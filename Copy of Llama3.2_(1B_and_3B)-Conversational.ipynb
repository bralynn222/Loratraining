{"cells":[{"cell_type":"markdown","metadata":{"id":"ILAoS8LLxgZ_"},"source":["To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n","<div class=\"align-center\">\n","<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n","<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n","<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n","</div>\n","\n","This notebook will show you how to finetune `openlm-research/open_llama_3b_v2` using Unsloth.\n","\nYou will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"]},{"cell_type":"markdown","metadata":{"id":"kEpdGZAsxgaB"},"source":["### News"]},{"cell_type":"markdown","metadata":{"id":"T1mhWl6sxgaB"},"source":["Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n","\n","Read our **[Qwen3 Guide](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n","\n","Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"]},{"cell_type":"markdown","metadata":{"id":"IKWoQsGixgaB"},"source":["### Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tj8yuPNqxgaB"},"outputs":[],"source":["\n","    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","    !pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes\n"]},{"cell_type":"markdown","metadata":{"id":"OQQpA9K6xgaC"},"source":["### Load Model & Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mDvVVYYjxgaC"},"outputs":[],"source":["from unsloth import FastLanguageModel\n","import torch\n","\n","# The model's default max sequence length is 2048.\n","# Unsloth automatically handles RoPE scaling for longer sequences.\n","max_seq_length = 2048\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"openlm-research/open_llama_3b_v2\",\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",")"]},{"cell_type":"markdown","metadata":{"id":"SXd9bTZd1aaL"},"source":["We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bZsfBuZDeCL"},"outputs":[],"source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"]},{"cell_type":"markdown","metadata":{"id":"vITh0KVJ10qX"},"source":["<a name=\"Data\"></a>\n","### Data Prep\n","We now use the Alpaca format for instruction tuning. The `openlm-research/open_llama_3b_v2` is a base model, so it doesn't have a specific chat format. The Alpaca format is a widely used, simple, and effective template for tuning base models. It looks like this:\n","\n","```\n","### Instruction:\n","{user_prompt}\n","\n","### Response:\n","{assistant_response}\n","```\n","\n","We'll use the same [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset, but we will format it using the Alpaca template. We use our `get_chat_template` function to set the correct chat template on the tokenizer. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3` and more."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjY75GoYUCB8"},"outputs":[],"source":["from unsloth.chat_templates import get_chat_template\n","\n","tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template = \"alpaca\", # For OpenLlama, we use the Alpaca template\n","    # You can also use \"vicuna\" or \"llama-2\" if you prefer.\n",")\n","\n","def formatting_prompts_func(examples):\n","    convos = examples[\"conversations\"]\n","    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n","    return { \"text\" : texts, }\n","pass\n","\n","from datasets import load_dataset\n","dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"]},{"cell_type":"markdown","metadata":{"id":"K9CBpiISFa6C"},"source":["We now use `standardize_sharegpt` to convert ShareGPT style datasets into HuggingFace's generic format. This changes the dataset from looking like:\n","```\n","{\"from\": \"system\", \"value\": \"You are an assistant\"}\n","{\"from\": \"human\", \"value\": \"What is 2+2?\"}\n","{\"from\": \"gpt\", \"value\": \"It's 4.\"}\n","```\n","to\n","```\n","{\"role\": \"system\", \"content\": \"You are an assistant\"}\n","{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n","{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oPXzJZzHEgXe"},"outputs":[],"source":["from unsloth.chat_templates import standardize_sharegpt\n","dataset = standardize_sharegpt(dataset)\n","dataset = dataset.map(formatting_prompts_func, batched = True,)"]},{"cell_type":"markdown","metadata":{"id":"ndDUB23CGAC5"},"source":["We look at how the conversations are structured for item 1:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGFzmplrEy9I"},"outputs":[],"source":["dataset[1][\"conversations\"]"]},{"cell_type":"markdown","metadata":{"id":"GfzTdMtvGE6w"},"source":["And we see how the chat template transformed these conversations into the Alpaca format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhXv0xFMGNKE"},"outputs":[],"source":["print(dataset[1][\"text\"])"]},{"cell_type":"markdown","metadata":{"id":"idAEIeSQ3xdS"},"source":["<a name=\"Train\"></a>\n","### Train the model\n","Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"95_Nn-89DhsL"},"outputs":[],"source":["from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","        warmup_steps = 5,\n","        max_steps = 60,\n","        learning_rate = 2e-4,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")"]},{"cell_type":"markdown","metadata":{"id":"C_sGp5XlG6dq"},"source":["We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"juQiExuBG5Bt"},"outputs":[],"source":["from unsloth.chat_templates import train_on_responses_only\n","\n","# For Alpaca, the response starts with \"### Response:\".\n","trainer = train_on_responses_only(\n","    trainer,\n","    response_template = \"### Response:\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"2ejIt2xSNKKp"},"outputs":[],"source":["#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqxqAZ7KJ4oL"},"outputs":[],"source":["trainer_stats = trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"pCqnaKmlO1U9"},"outputs":[],"source":["#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"]},{"cell_type":"markdown","metadata":{"id":"ekOmTR1hSNcr"},"source":["<a name=\"Inference\"></a>\n","### Inference\n","Let's run the model! You can change the instruction and input - leave the output blank!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kR3gIAX-SM2q"},"outputs":[],"source":["# The tokenizer is already configured with the Alpaca chat template.\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\nmessages = [\n    {\"from\": \"human\", \"value\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n]\n\n# We have to re-standardize the data format to use `apply_chat_template`\nfrom unsloth.chat_templates import standardize_sharegpt\nmessages = standardize_sharegpt(messages)\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\noutputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\nprint(tokenizer.batch_decode(outputs)[0])"]},{"cell_type":"markdown","metadata":{"id":"CrSvZObor0lY"},"source":[" You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2pEuRb1r2Vg"},"outputs":[],"source":["FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# The prompt format should be consistent with the Alpaca template\nmessages = [\n    {\"from\": \"human\", \"value\": \"Write a short story about a dragon who loves to bake.\"},\n]\n\n# We have to re-standardize the data format to use `apply_chat_template`\nfrom unsloth.chat_templates import standardize_sharegpt\nmessages = standardize_sharegpt(messages)\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)"]},{"cell_type":"markdown","metadata":{"id":"uMuVrWbjAzhc"},"source":["<a name=\"Save\"></a>\n","### Saving, loading finetuned models\n","To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n","\n","**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"upcOlWe7A1vc"},"outputs":[],"source":["model.save_pretrained(\"lora_model\") # Local saving\ntokenizer.save_pretrained(\"lora_model\")\n# model.push_to_hub(\"your_name/lora_model\", token = \"hf_...\") # Online saving\n# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"hf_...\") # Online saving"]},{"cell_type":"markdown","metadata":{"id":"AEEcJ4qfC7Lp"},"source":["Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKX_XKs_BNZR"},"outputs":[],"source":["if False:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n    messages = [\n        {\"from\": \"human\", \"value\": \"What is a famous tall tower in Paris?\"},\n    ]\n    \n    # We have to re-standardize the data format to use `apply_chat_template`\n    from unsloth.chat_templates import standardize_sharegpt\n    messages = standardize_sharegpt(messages)\n\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        tokenize = True,\n        add_generation_prompt = True, # Must add for generation\n        return_tensors = \"pt\",\n    ).to(\"cuda\")\n\n    from transformers import TextStreamer\n    text_streamer = TextStreamer(tokenizer)\n    _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"]},{"cell_type":"markdown","metadata":{"id":"QQMjaNrjsU5_"},"source":["You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFfaXG0WsQuE"},"outputs":[],"source":["if False:\n    # I highly do NOT suggest - use Unsloth if possible\n    from peft import AutoPeftModelForCausalLM\n    from transformers import AutoTokenizer\n    model = AutoPeftModelForCausalLM.from_pretrained(\n        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n        load_in_4bit = load_in_4bit,\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"]},{"cell_type":"markdown","metadata":{"id":"f422JgM9sdVT"},"source":["### Saving to float16 for VLLM\n\nWe also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHjt_SMYsd3P"},"outputs":[],"source":["# Merge to 16bit\nif False: model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\",)\nif False: model.push_to_hub_merged(\"your_name/your_model_merged\", tokenizer, save_method = \"merged_16bit\", token = \"hf_...\")\n\n# Merge to 4bit\nif False: model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_4bit\",)\nif False: model.push_to_hub_merged(\"your_name/your_model_merged\", tokenizer, save_method = \"merged_4bit\", token = \"hf_...\")\n\n# Just LoRA adapters\n# model.save_pretrained(\"lora_model\") # Local saving\n# tokenizer.save_pretrained(\"lora_model\")\n# model.push_to_hub(\"your_name/lora_model\", token = \"hf_...\") # Online saving\n# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"hf_...\") # Online saving"]},{"cell_type":"markdown","metadata":{"id":"TCv4vXHd61i7"},"source":["### GGUF / llama.cpp Conversion\n","To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n","\n","Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n","* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n","* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n","* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n","\n","[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FqfebeAdT073"},"outputs":[],"source":["# Save to 8bit Q8_0\nif False: model.save_pretrained_gguf(\"model\", tokenizer,)\nif False: model.push_to_hub_gguf(\"your_name/gguf_model\", tokenizer, token = \"hf_...\")\n\n# Save to 16bit GGUF\nif False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\nif False: model.push_to_hub_gguf(\"your_name/gguf_model\", tokenizer, quantization_method = \"f16\", token = \"hf_...\")\n\n# Save to q4_k_m GGUF\nif False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\nif False: model.push_to_hub_gguf(\"your_name/gguf_model\", tokenizer, quantization_method = \"q4_k_m\", token = \"hf_...\")"]},{"cell_type":"markdown","metadata":{"id":"Flc-ij7dxgaH"},"source":["Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n","\n","And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n","\n","Some other links:\n","1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n","2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n","3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n","6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n","\n","<div class=\"align-center\">\n","  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n","  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n","  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n","\n","  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n","</div>\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}{"cells":[{"cell_type":"markdown","metadata":{"id":"ILAoS8LLxgZ_"},"source":["To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n","<div class=\"align-center\">\n","<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n","<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n","<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n","</div>\n","\n","This notebook will show you how to finetune `openlm-research/open_llama_3b_v2` using Unsloth.\n","\nYou will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"]},{"cell_type":"markdown","metadata":{"id":"kEpdGZAsxgaB"},"source":["### News"]},{"cell_type":"markdown","metadata":{"id":"T1mhWl6sxgaB"},"source":["Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n","\n","Read our **[Qwen3 Guide](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n","\n","Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"]},{"cell_type":"markdown","metadata":{"id":"IKWoQsGixgaB"},"source":["### Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tj8yuPNqxgaB"},"outputs":[],"source":["\n","    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","    !pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes\n"]},{"cell_type":"markdown","metadata":{"id":"OQQpA9K6xgaC"},"source":["### Load Model & Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mDvVVYYjxgaC"},"outputs":[],"source":["from unsloth import FastLanguageModel\n","import torch\n","\n","# The model's default max sequence length is 2048.\n","# Unsloth automatically handles RoPE scaling for longer sequences.\n","max_seq_length = 2048\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"openlm-research/open_llama_3b_v2\",\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",")"]},{"cell_type":"markdown","metadata":{"id":"SXd9bTZd1aaL"},"source":["We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bZsfBuZDeCL"},"outputs":[],"source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"]},{"cell_type":"markdown","metadata":{"id":"vITh0KVJ10qX"},"source":["<a name=\"Data\"></a>\n","### Data Prep\n","We now use the Alpaca format for instruction tuning. The `openlm-research/open_llama_3b_v2` is a base model, so it doesn't have a specific chat format. The Alpaca format is a widely used, simple, and effective template for tuning base models. It looks like this:\n","\n","```\n","### Instruction:\n","{user_prompt}\n","\n","### Response:\n","{assistant_response}\n","```\n","\n","We'll use the same [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset, but we will format it using the Alpaca template. We use our `get_chat_template` function to set the correct chat template on the tokenizer. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3` and more."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjY75GoYUCB8"},"outputs":[],"source":["from unsloth.chat_templates import get_chat_template\n","\n","tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template = \"alpaca\", # For OpenLlama, we use the Alpaca template\n","    # You can also use \"vicuna\" or \"llama-2\" if you prefer.\n",")\n","\n","def formatting_prompts_func(examples):\n","    convos = examples[\"conversations\"]\n","    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n","    return { \"text\" : texts, }\n","pass\n","\n","from datasets import load_dataset\n","dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"]},{"cell_type":"markdown","metadata":{"id":"K9CBpiISFa6C"},"source":["We now use `standardize_sharegpt` to convert ShareGPT style datasets into HuggingFace's generic format. This changes the dataset from looking like:\n","```\n","{\"from\": \"system\", \"value\": \"You are an assistant\"}\n","{\"from\": \"human\", \"value\": \"What is 2+2?\"}\n","{\"from\": \"gpt\", \"value\": \"It's 4.\"}\n","```\n","to\n","```\n","{\"role\": \"system\", \"content\": \"You are an assistant\"}\n","{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n","{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oPXzJZzHEgXe"},"outputs":[],"source":["from unsloth.chat_templates import standardize_sharegpt\n","dataset = standardize_sharegpt(dataset)\n","dataset = dataset.map(formatting_prompts_func, batched = True,)"]},{"cell_type":"markdown","metadata":{"id":"ndDUB23CGAC5"},"source":["We look at how the conversations are structured for item 1:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGFzmplrEy9I"},"outputs":[],"source":["dataset[1][\"conversations\"]"]},{"cell_type":"markdown","metadata":{"id":"GfzTdMtvGE6w"},"source":["And we see how the chat template transformed these conversations into the Alpaca format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhXv0xFMGNKE"},"outputs":[],"source":["print(dataset[1][\"text\"])"]},{"cell_type":"markdown","metadata":{"id":"idAEIeSQ3xdS"},"source":["<a name=\"Train\"></a>\n","### Train the model\n","Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"95_Nn-89DhsL"},"outputs":[],"source":["from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","        warmup_steps = 5,\n","        max_steps = 60,\n","        learning_rate = 2e-4,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")"]},{"cell_type":"markdown","metadata":{"id":"C_sGp5XlG6dq"},"source":["We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"juQiExuBG5Bt"},"outputs":[],"source":["from unsloth.chat_templates import train_on_responses_only\n","\n","# For Alpaca, the response starts with \"### Response:\".\n","trainer = train_on_responses_only(\n","    trainer,\n","    response_template = \"### Response:\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"2ejIt2xSNKKp"},"outputs":[],"source":["#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqxqAZ7KJ4oL"},"outputs":[],"source":["trainer_stats = trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"pCqnaKmlO1U9"},"outputs":[],"source":["#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"]},{"cell_type":"markdown","metadata":{"id":"ekOmTR1hSNcr"},"source":["<a name=\"Inference\"></a>\n","### Inference\n","Let's run the model! You can change the instruction and input - leave the output blank!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kR3gIAX-SM2q"},"outputs":[],"source":["# The tokenizer is already configured with the Alpaca chat template.\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\nmessages = [\n    {\"from\": \"human\", \"value\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n]\n\n# We have to re-standardize the data format to use `apply_chat_template`\nfrom unsloth.chat_templates import standardize_sharegpt\nmessages = standardize_sharegpt(messages)\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\noutputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\nprint(tokenizer.batch_decode(outputs)[0])"]},{"cell_type":"markdown","metadata":{"id":"CrSvZObor0lY"},"source":[" You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2pEuRb1r2Vg"},"outputs":[],"source":["FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# The prompt format should be consistent with the Alpaca template\nmessages = [\n    {\"from\": \"human\", \"value\": \"Write a short story about a dragon who loves to bake.\"},\n]\n\n# We have to re-standardize the data format to use `apply_chat_template`\nfrom unsloth.chat_templates import standardize_sharegpt\nmessages = standardize_sharegpt(messages)\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)"]},{"cell_type":"markdown","metadata":{"id":"uMuVrWbjAzhc"},"source":["<a name=\"Save\"></a>\n","### Saving, loading finetuned models\n","To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n","\n","**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"upcOlWe7A1vc"},"outputs":[],"source":["model.save_pretrained(\"lora_model\") # Local saving\ntokenizer.save_pretrained(\"lora_model\")\n# model.push_to_hub(\"your_name/lora_model\", token = \"hf_...\") # Online saving\n# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"hf_...\") # Online saving"]},{"cell_type":"markdown","metadata":{"id":"AEEcJ4qfC7Lp"},"source":["Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKX_XKs_BNZR"},"outputs":[],"source":["if False:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n    messages = [\n        {\"from\": \"human\", \"value\": \"What is a famous tall tower in Paris?\"},\n    ]\n    \n    # We have to re-standardize the data format to use `apply_chat_template`\n    from unsloth.chat_templates import standardize_sharegpt\n    messages = standardize_sharegpt(messages)\n\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        tokenize = True,\n        add_generation_prompt = True, # Must add for generation\n        return_tensors = \"pt\",\n    ).to(\"cuda\")\n\n    from transformers import TextStreamer\n    text_streamer = TextStreamer(tokenizer)\n    _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"]},{"cell_type":"markdown","metadata":{"id":"QQMjaNrjsU5_"},"source":["You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFfaXG0WsQuE"},"outputs":[],"source":["if False:\n    # I highly do NOT suggest - use Unsloth if possible\n    from peft import AutoPeftModelForCausalLM\n    from transformers import AutoTokenizer\n    model = AutoPeftModelForCausalLM.from_pretrained(\n        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n        load_in_4bit = load_in_4bit,\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"]},{"cell_type":"markdown","metadata":{"id":"f422JgM9sdVT"},"source":["### Saving to float16 for VLLM\n\nWe also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHjt_SMYsd3P"},"outputs":[],"source":["# Merge to 16bit\nif False: model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\",)\nif False: model.push_to_hub_merged(\"your_name/your_model_merged\", tokenizer, save_method = \"merged_16bit\", token = \"hf_...\")\n\n# Merge to 4bit\nif False: model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_4bit\",)\nif False: model.push_to_hub_merged(\"your_name/your_model_merged\", tokenizer, save_method = \"merged_4bit\", token = \"hf_...\")\n\n# Just LoRA adapters\n# model.save_pretrained(\"lora_model\") # Local saving\n# tokenizer.save_pretrained(\"lora_model\")\n# model.push_to_hub(\"your_name/lora_model\", token = \"hf_...\") # Online saving\n# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"hf_...\") # Online saving"]},{"cell_type":"markdown","metadata":{"id":"TCv4vXHd61i7"},"source":["### GGUF / llama.cpp Conversion\n","To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n","\n","Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n","* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n","* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n","* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n","\n","[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FqfebeAdT073"},"outputs":[],"source":["# Save to 8bit Q8_0\nif False: model.save_pretrained_gguf(\"model\", tokenizer,)\nif False: model.push_to_hub_gguf(\"your_name/gguf_model\", tokenizer, token = \"hf_...\")\n\n# Save to 16bit GGUF\nif False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\nif False: model.push_to_hub_gguf(\"your_name/gguf_model\", tokenizer, quantization_method = \"f16\", token = \"hf_...\")\n\n# Save to q4_k_m GGUF\nif False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\nif False: model.push_to_hub_gguf(\"your_name/gguf_model\", tokenizer, quantization_method = \"q4_k_m\", token = \"hf_...\")"]},{"cell_type":"markdown","metadata":{"id":"Flc-ij7dxgaH"},"source":["Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n","\n","And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n","\n","Some other links:\n","1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n","2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n","3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n","6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n","\n","<div class=\"align-center\">\n","  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n","  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n","  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n","\n","  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n","</div>\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}